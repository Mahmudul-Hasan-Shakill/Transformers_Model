## Transformers Model
Transformers provides APIs to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you time from training a model from scratch. The models can be used across different modalities such as:

- ğŸ“ **Text**: text classification, information extraction, question answering, summarization, translation, and text generation in over 100 languages.
- ğŸ–¼ï¸ **Images**: image classification, object detection, and segmentation.
- ğŸ—£ï¸ **Audio**: speech recognition and audio classification.
- ğŸ™ **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

To learn more, please visit [Hugging Face](https://huggingface.co/docs/transformers/index) and see documentation.

## Authors

[@Mahmudul Hasan Shakil](https://github.com/Mahmudul-Hasan-Shakill/)

[![MIT License](https://img.shields.io/apm/l/atomic-design-ui.svg?)](https://github.com/tterb/atomic-design-ui/blob/master/LICENSEs)
